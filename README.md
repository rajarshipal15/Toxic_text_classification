# Toxic_text_classification
 We have performed text classification on the Kaggle Toxic Comments Challenge data. After EDA and data cleaning, we  used Glove word embeddings and a Bi-LSTM as our model. After 5 epochs of training we obtained a validation AUC of 0.984.   
